\documentclass[11pt]{article}

\usepackage[authoryear]{natbib}
\usepackage{xcolor}
\usepackage[T1]{fontenc}
\usepackage{fourier}
\usepackage[utf8]{inputenc}
\usepackage[hidelinks]{hyperref}
\usepackage{slashbox}
\usepackage{graphicx}
\usepackage{url}


\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}\PackageWarning{TODO:}{#1!}}
\newcommand{\note}[1]{\textcolor{red}{[NOTE: #1]}\PackageWarning{NOTE:}{#1!}}
\newcommand*{\np}{\par\noindent\newline}

\title{Agent Based Modelling of the Formation of Social Preferences}
\author{S Pardy}
\begin{document}
\begin{titlepage}
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for horizontal lines, change thickness here
	
	\center % Centre everything on the page
	
	%------------------------------------------------
	%	Headings
	%------------------------------------------------
	
	\textsc{\LARGE Monash University}\\[1.5cm]	
	\textsc{\Large Bachelor of Science (Honours)}\\[0.5cm] % Major heading such as course name
	
	\textsc{\large Computational Science}\\[0.5cm] % Minor heading such as course title
	
	%------------------------------------------------
	%	Title
	%------------------------------------------------
	
	\HRule\\[0.4cm]
	
	{\huge\bfseries Agent Based Modelling of the Formation of Social Preferences}\\[0.4cm]
	
    \HRule\\[1.5cm]
    

	
	{\huge Literature Review }\\[0.4cm]
	
    % \HRule\\[1.5cm]
    
    

   \vfill\vfill

	%------------------------------------------------
	%	Author(s)
	%------------------------------------------------
	
	\begin{minipage}{0.4\textwidth}
		\begin{flushleft}
			\large
			\textit{Author}\\
			Sam \textsc{Pardy}
		\end{flushleft}
	\end{minipage}
	~
	\begin{minipage}{0.4\textwidth}
		\begin{flushright}
			\large 
			\textit{Student ID}\\
		25940783   
		\end{flushright}
	\end{minipage}
	\vfill
	\vfill
	{\large
	\textit{Supervisor}\\
	Dr. Julian \textsc{García}}
	
	%------------------------------------------------
	%	Date
	%------------------------------------------------

	\vfill\vfill\vfill % Position the date 3/4 down the remaining page
	{\large\today} % Date
	\vfill % Push the date up 1/4 of the remaining page
	
\end{titlepage}
\hypersetup{colorlinks, citecolor=black, filecolor=black, linkcolor=black urlcolor=black}

\newpage
\tableofcontents
\newpage
\section{Introduction}\label{intro}
Understanding cooperation is greatly important to our understanding of why people behave the way they do.
Understanding the kinds of environments that lead agents to behave cooperatively will allow us to create systems that
encourage or engender this behaviour. The fact that some of the biggest problems facing humanity require cooperation in
support of solutions is one of the factors that pushes the quest to understand cooperation from one of purely
intellectual value to a thing of great practical importance. Despite the substantial amount of existing research in the area, understanding cooperation is still an open question.

\np The aim of this review is to explore literature concerned with theories of the formation of social preferences and
agent based models as applied to the formation of social preferences. This area of research is rich, with a wealth of
published work in existence, as such only a portion is be covered in this piece. Particular interest is be paid to
results achieved in \citet{alger_homo_2013} and the surrounding literature, as well as the potential of extending such
static analysis in dynamic models. With this in mind, it is necessary to explore the existing literature that supports
and extends the work of \citet{alger_homo_2013} as well as work interested in the dynamic modelling of similar
representations of preference formation.

\np Broadly, I begin this review with a discussion of the techniques used widely in statically modelling social interactions, before shifting to a review of the literature surrounding dynamic, agent based modelling.
Finally, I look at the current, state-of-the-art work in social preference formation as well as work that
synthesises game theory research and dynamic modelling.


\section{Game Theoretic Background}\label{social}
\subsection{Games}\label{games}
In order to think about and investigate the situations in which agents might act cooperatively, we need to be able to
model the interactions themselves. These `social' interactions can be characterized as any interaction in which the
payoff of one player is dependent not just on their own action but on the action of one or more other players involved \citep{angner_course_2012}. Game Theory is the study of such social interactions. More formally, according to
\citet{osborne_course_1994} a game includes:
\begin{itemize}
	\item the finite set, N, of players;
	\item for each player, i $\in$ N, a set $A_i$, of actions available to that
	player $N_i$;
	\item and, for each player, i $\in$ N, the set $P_i$ of pay-offs associated
	with the available actions.
\end{itemize}
It is assumed that the players are motivated solely by the payoffs on offer and, in pursuit of these payoffs, act
rationally.
\np An important game in the investigation of cooperative games is the Prisoner's Dilemma. 
The Prisoner's Dilemma is described as as a situation in which two prisoners are being separately questioned in the hope
that they turn on one another. If neither talk they will both receive a minimal jail sentence, while if one talk
(defects) and the other stays silent (cooperates) the defector will go free and the cooperator will receive the maximum
sentence. If both talk, they will both receive a medium length sentence.  The payoff structure, where high payoff
corresponds to less prison time, is as follows:
\begin{center}
	\begin{tabular}{|l||*{5}{c|}}\hline
	 \label{prisoner_payoff}
	 \backslashbox{Prisoner A}{Prisoner B}
	 &\makebox[7em]{Cooperate}&\makebox[7em]{Defect}\\\hline\hline
	 Cooperate & 3, 3 & 0, 5\\\hline
	 Defect & 5, 0 & 1, 1 \\\hline
	 \end{tabular}
 \end{center}\mbox{}\\
 Influential game theorist Robert Axelrod holds that examples of the Prisoner's Dilemma can be seen in the military
 decisions of the cold war United States and Soviet Union as well as in their respective decisions regarding defence
 budgets \citep{axelrod_effective_1980}. Axelrod analyses the Prisoner's Dilemma as capturing the "tension between
 individual rationality ... and group rationality" \cite[p. 4]{axelrod_effective_1980}. This is captured in the payoff
 structure by incentivising both sides to be selfish while also awarding high payoff to both sides when they cooperate.

\np When we talk about how agents play these games, we talk about `strategies'. A strategy is a complete plan of how to
play the game that includes actions under each possible circumstance \citep{angner_course_2012}. A strategy of the kind
discussed above in relation to the prisoner's dilemma is often called a \textit{pure} strategy. A pure strategy is a
deterministic plan of action for the entire game. Conversely, a player implementing a \textit{mixed} strategy will play
each of the pure strategies with some probability (this probability could be zero for some strategies). So, in the
Prisoner's Dilemma there are two pure strategies available to each player: Stay Silent or Talk, alternatively, Cooperate
or Defect. A mixed strategy would playing each of these two strategies with some probability.

\np The next step in the analysis is to attempt to predict what strategies players will play when encountering one
another in a game; the way that this is done is by finding \textit{Nash Equilibria}. If, when two players are engaged in
a game, neither player can improve their payoff by changing their strategy while their opponent's strategy remains the
same the players are said to be playing a Nash Equilibrium \citep{angner_course_2012}. Given our assumption about
players acting rationally, by extension, it can be concluded that, when games are repeated, play will end up in a state
of Nash Equilibria \citep{kalai_rational_1993}. In the Prisoner's Dilemma, there is only one Nash Equilibria - both
defect. One of the most influential papers in game theoretic literature is \citep{nash_equilibrium_1950}, in which John
Nash concludes that, when mixed strategies are permitted, any game with a finite strategy set will have at least one equilibrium point. So, when we're analysing the various games in pursuit of cooperation, we can be sure that an equilibrium exists.

\np It should be clear from the discussion so far that any model of the Prisoner's Dilemma that we set up using 
the tools mentioned so far will lead to un-cooperative behaviour. Two rational players, motivated by payoffs will
engaged in a Prisoner's Dilemma will eventually play the Nash-Equilibrium strategy of both-defect. So how do reconcile
such a model with cooperative behaviour? In the literature this is done by introducing preferences. In this context, a
preference is a relation between two things \citep{angner_course_2012}; for example, Jane \textit{prefers} cycling to
walking. Of course, preferences such as this are subjective - Jill might prefer walking to cycling - but since our
agents are rational, they must also be rational about their preferences. When this is the case then preferences are
transitive and this allows us to order all alternatives in a list from most preferred to least. This is trivial when an
example is given - If Jane prefers cycling to walking, and she prefers walking to horse riding then we can give a
preferences ordering of cycling, walking, horse riding. Preferences are integral to this area of research as they allow
us to bridge the gap between subjectivity and rationality; a rational agent can have subjective preferences that may go
against their objective self interest but still rationally pursue the outcomes that they most prefer.

\np In the pursuit of cooperation, we are interested in `social' preferences. A preference is considered to be social if
it is a relation that considers the satisfaction of others as well (or even in place of) the satisfaction gained by
oneself \citep{angner_course_2012}. Another way to phrase this definition is to say that an agent with social
preferences is motivated by something other than pure self-interest. Traditionally, much of economics has been
predicated on the assumption that rational actors are motivated solely by self interest, but it has been shown that this
analysis fails to explain large portions of observed behaviour; behaviour that can be explained when social preferences
are considered \citep{fehr_why_2002}. Being motivated by more than just self-interest is not necessarily a `good' thing
though - the definition says nothing about whether a social preference assigns positive (altruistic) or negative
(jealous) weight to another agents satisfaction. In fact, a mixture of the two, known as reciprocal fairness, is an
important preference type \citep{fehr_why_2002,guth_evolutionary_1995}. An agent who is said to act according to
reciprocal altruistically rewards those who cooperate while also going out of their way (i.e. reducing their own
satisfaction) to punish those who defect; this second part is often called altruistic punishment
\citep{fehr_altruistic_2002}.

\np The way that we represent preferences numerically is with utility. The logic being that if an agent prefers one
alternative to another, then they will achieve a higher utility when their preferred alternative eventuates. So, each
alternative in a particular set can be determined as having some level of utility associated with it based on the
agent's preference ordering. This determination can be done via a utility function \citep{angner_course_2012}. A utility
function maps from objective, material outcomes to the subjective utility that an agent achieves when that outcome
occurs. So, given a two player game and the utility functions of both players we can use these utility functions
to write down a new game that is structured around the utility that each player receives in each potential outcome
rather that the payoff outcome itself.

\np For example, let's assume we have agents who have social preferences about the outcome of the game they're playing.
In the following utility function, the player is some kind of utilitarian who cares about their opponent as much as themselves:
\begin{center}
	\(U(P_i, P_o) = \frac{1}{2} \times P_i + \frac{1}{2} \times P_o\) 
\end{center}
Where $U$ is the utility achieved, $P_i$ is my payoff and $P_o$ is my opponent's payoff.
If we assume that both players in a Prisoner's Dilemma have this utility function, the payoff structure is significantly
changed. The \textit{utility} based payoff matrix now looks like:
\newline
\begin{center}
	\begin{tabular}{|l||*{5}{c|}}\hline
		\label{utility_prisoner}
		\backslashbox{Prisoner A}{Prisoner B}
		&\makebox[7em]{Cooperate}&\makebox[7em]{Defect}\\\hline\hline
		Cooperate & 4.5, 4.5 & 2.5, 2.5\\\hline
		Defect & 2.5, 2.5 & 1, 1 \\\hline
	\end{tabular}
\end{center}\mbox{}\\
Now, unlike the original Prisoner's Dilemma, both players cooperating is an equilibrium point.

\np It's here though, that we run into a problem. The players are now engaging in a game of incomplete information.
In games where players only care about the payoff that they achieve it is often reasonable to assume that the players
also know about the payoffs that their opponents might receive. In other words, the players know about the game that
they're playing. When we take into account their subjective preferences, it does not seem plausible to assume that the
players would know about each others internal desires. This is important because when deciding which action to take,
players must reason about and take into account the action that their opponent will take. When this information is
unknown, it unclear how players should analyse and predict their opponent's action; games of this kind are often called Bayesian Games.

\np Along with the basic building blocks of games listed in the beginning this section, Bayesian games can be understood
by introducing the concept of \textit{types} \citep[~p. 167]{shoham_multiagent_2008}. Faced with the same scenario,
players of different types will receive different payoffs, and so will act differently to one another. Crucially,
players do not know the type of their opponent and so must reason about their opponent's action with this uncertainty in
mind. This is done via players having \textit{beliefs} about the likelihood of facing the different possible types. A
commonly used example to illustrate these concepts is that of a soccer goalkeeper facing a striker. The goalkeeper
doesn't know whether the striker is left or right footed (ala their type) but has beliefs about how likely it is that
she is facing a left or right footed striker (say a 50-50 chance), based on these beliefs, the goal keeper act in a way
so as to maximise her \textit{expected} payoff. 
A crucial assumption in support of this analysis is the common-prior assumption; this assumption states that the players
don't just have beliefs about the type of player they're facing, but also beliefs about that player's beliefs, and so on
along an infinite hierarchy of beliefs. This is a strong assumption, but it is one that is widely made in the literature
surrounding Bayesian games \citep[~p. 164]{shoham_multiagent_2008}. The solution concept for Bayesian games, Bayesian
Nash Equilibrium, is similar to that of standard Nash Equilibria with the additional complexity of players attempting to
maximise their expected payoff given their believes about the types that they're facing, and that the best response set
must provide a strategy for all possible types \citet{shoham_multiagent_2008}. Given the assumptions needed to support
this solution concept, it is possible that it has limited applicability in practical scenarios.
An alternative described in \citet{kalai_rational_1993} holds that in the presence of incomplete information, when
individuals have correct beliefs about the strategies of other players and games are repeated infinitely, play will end
up in the same state as if complete information was present. This suggests it is possible, under certain circumstances,
to analyse Bayesian games as though players had complete information.


\subsection{Evolutionary Game Theory}\label{evo_games}
Modelling the \textit{formation} of preferences is the purview of evolutionary game theory. Evolutionary game theory (as
the name suggests) is the result of the combination of traditional game theory and theories of biological evolution to
explore the changing of behaviour over time \citep{tanimoto_fundamentals_2015}. The key concepts of evolutionary game
theory include the key concepts of evolutionary biology: fitness, mutation, selection. When dealing with the
formation of preferences, a player's action is motivated by their preference but their \textit{fitness} is related to
their objective payoff. For example, many people have a strong preference to stay on the couch rather than going for a
run. In general, there is very little material payoff that can be gained from staying on the couch whereas exercising
has significant long term health benefits. So, when someone chooses to stay and lie on the couch rather than exercise it
can, in most cases, be seen as an example of action being motivated by subjective utility rather than material payoff.

\todo{add a sentence or two about mechanics of evolutionary game theory}

\np Analogous to the Nash Equilibrium solution concept in pure game theory, is the idea of evolutionary stable
strategies (ESS) in evolutionary game theory. A strategy is said to be evolutionary stable if it achieves a higher
payoff (fitness) than that which is achieved by every mutant strategy when the population share of the mutant strategy
is small \citep{weibull_introduction_1992}. That is, strategy $s$ achieves a higher payoff when playing against itself
and against some mutant strategy $m$ than $m$ does playing in the same environment \citep{shoham_multiagent_2008}. In
\citep{weibull_introduction_1992} Weibull also notes that the concept of evolutionary stable strategies is more
stringent than Nash equilibrium: if a strategy is evolutionary stable it is also a Nash equilibrium response to itself
as well as fulfilling the other requirements that ESS imposes.

\section{Computer Modelling \& Game Theory}\label{agent}
Much of the existing literature surrounding evolutionary game theory is concerned with performing static analysis of
evolutionary models \citep{alger_homo_2013,hetzer_evolutionary_2013,newton_preferences_2017}. and, while performing
computer simulations of evolutionary game theory processes is not an entirely novel area, there remains are a large
array of unexplored territory. A basic and useful technique in this area is \textit{genetic programming (GP)}. Not
limited to evolutionary game theory, GP abstracts the search and optimization processes of evolution to apply them to a
wide range of hard to solve problems. When used as an optimization technique, GP improves the population of potential
solutions generation by generation by stochastically transforming these solutions and assessing there fitness
\citep{poli_field_2008}. The fitness functions referenced in \citep{poli_field_2008} are varied and problem specific, in
our context a fitness function will be based somehow on the payoff that a player achieves. In discussing GP as applied
to game theory, Robert Axelrod described the technique as a "highly effective" method for exploring strategies \cite[p.
~23]{axelrod_effective_1980}. 

\np Another important technique in this area is agent based modelling (ABM). ABM is a family of computational modelling
that attempts to draw macro-level conclusions about systems or environments by modelling actions of individual agents.
ABM is an important modelling technique, not least because rather than describing a system as a whole it describes the
actions that individual agents can take. In this way, ABMs are "less simplified" than other forms of modelling and allow
us to observe potentially more complicated dynamics \cite[p. ~25]{railsback_agent-based_2011}. Of course, the central
tenet of the ABM is the agent. Wide spread agreement about what precisely an agent is does not exist, however there are
several key characteristics that are largely universal \citep{heppenstall_introduction_2012}. These are: 
\begin{itemize}
	\item Autonomy: agents act independently, without influence from some central controller.
	\item Heterogeneity: diversity can exist within the population of agents. Loose `types' of agents can exist but
	these groups are formed over time and can be considered amalgamations of similar autonomous agents.
	\item Active: agents contribute to the environment around them; they apply independent influence on the simulation.
\end{itemize}
\citep{heppenstall_introduction_2012}

\np Another key aspect of ABM is the set of rules that govern the environment. Each object defined within the model
(whether active or inactive) has rules that define their behaviour and interaction with other objects. Rules can be be
derived from published literature, data analysis or even just stated as assumptions about the model
\citep{heppenstall_introduction_2012}. For example, agents of the kind discussed towards the end of section \ref{games}
would all act in accordance with a rule to maximise utility. 

\np Work at the nexus of computational modelling and game theory has been undertaken in \citet{garcia_no_2018}. This
paper finds that computer simulation agrees with game theoretic analysis of repeated prisoner's dilemmas. Namely, that
no strategy can be entirely stable because for every equilibria certain sequences of mutants can destabilise them by
performing \textit{as well as} (not better than) the equilibrium strategy. With the system having `drifted' away from
the equilibrium point, the new resident strategy is now susceptible to invasion \citep{garcia_no_2018}. \todo{ extend
with reference to dynamic model}

\np Another example of existing work at this intersection is that undertaken in \citet{powers_modelling_2018}. Powers,
Ekárt and Lewis discuss the combination of agent based modelling and evolutionary game theory and characterize a
distinction between the two as being one of \textit{content based models} and \textit{value based models}. Standard
agent based modelling, the authors claim, captures the content of each agents strategy which determines their behaviour.
This can be considered the \textit{agent function}, as it maps from environmental stimuli to action \citep[~p.
69]{powers_modelling_2018}. In comparison, evolutionary game theory does not capture the behaviour of individual agents
but rather the strategies are considered as types with different levels of prevalence within a population. The
\textit{value} of these strategies is captured by some fitness functions that depend on the strategies and the
population structure. There is a sharp trade-off between the two kinds according to \citet{powers_modelling_2018}: ABM
does not capture the value of the actions that are carried out and it is difficult to know the value without executing
and observing the system; while EGT omits detail about the carrying out of strategies instead focusing on their overall
expected value. With this difference in mind, it can be seen that, while neither system can replace the other, attacking
the same problem with both kinds of model could be fruitful \citep{powers_modelling_2018}. This approach has
been successful, notably in \citep{axelrod_effective_1980}. It is claimed that the use of ABM can expand upon the results of existing EGT \citep[~p.71]{powers_modelling_2018}.

\section{The State-Of-The-Art}
With the research framework laid out in an introductory way, we can now move to a discussion of more cutting edge
literature in the area. Of course, formation of social preferences is broad and active research area that is impossible
to cover completely in a review such as this, we instead focus on only a narrow portion of interest within the overall space.

\np One significant contribution in the existing literature is \citet{alger_homo_2013}, in which the authors conclude
that when agents' preferences are their own private information, a particular utility function is an evolutionary stable
strategy; this is done within the context of model that hinges on some interesting assumptions. The first thing to note
is that rather than engaging in strategy evolution which is prevalent in the literature, \citet{alger_homo_2013} focus
on the evolution of preferences. The encounters that drive this preference evolution are not between uniformly matched
individuals, rather the matching process is assortative with the \textit{index of assortativity,} $\sigma \in [0, 1]$,
noted as a key parameter in the analysis. When the index of assortativity is zero then the matching process is
equivalent to uniformly random, but when it's greater than zero the matching process is biased towards individuals
matching with other of the same type. Second, as implied by agents' preferences being their own "private information",
the games that are analysed in this work are games of incomplete information and the concept of Bayesian Nash Equilibria
is employed to ascertain how players will act. Another point of interest from the model is that Alger and Weibull place
strict constraints on evolutionary stability and instability. To be considered an evolutionary stable strategy a
resident must achieve a strictly higher payoff than all potential mutants in \textit{all equilibria} \citep[~p.
2274]{alger_homo_2013}. Similarly, a strategy is evolutionary unstable if there exists some mutant against which the
strategy achieves strictly less payoff in all equilibria \citep[~p. 2275]{alger_homo_2013}.

\np The utility functions that \citet{alger_homo_2013} call \textit{Homo Moralis} are a weighted linear combination of
the expected payoffs of both players, they look like the following, in which $x$ and $y$ are types and $u_k(x, y)$ is
\textit{x's} utility when playing against \textit{y}:
\begin{center}
	$u_k(x, y) = (1 - k) \times \pi(x, y) + k \times \pi(x, x)$
\end{center}
for some $k \in [0, 1]$, the agents "degree of morality" \citep[~p 2274.]{alger_homo_2013}. The degree of morality can
be thought of as how much the individual `wants to do the right thing' - a degree of morality of $0$ corresponds to pure
selfishness, while a degree of $1$ corresponds to pure morality. An interesting observation that can be made of this
function is that the term that represents the opponent player's payoff -- that is, the term $\pi(x,x)$ -- treats the
opponent as if they were the same type as the player who's utility function it is regardless of what their belief or
actual type is. This would seem to ensure that any cooperation amongst agents of the same type is reciprocated. \todo{is this correct?}

\np The  utility function that the authors conclude to be evolutionary stable is one that they call \textit{Homo
Hamiltonensis}, in which the degree of morality is equal to the index of assortativity, $\sigma$:
\begin{center}
	$u_k(x, y) = (1 - \sigma) \times \pi(x, y) + \sigma \times \pi(x, x)$
\end{center}
When an individual who possesses this utility function interacts in an environment in which they are highly likely
to meet with another individual of the same type, they will act in a more `moral' way, taking into consideration the
payoff that their opponent's achieve. A significant accomplishment of this result is that it can reconcile existing
literature that suggests high levels of cooperation amongst kin, and low levels of cooperation in anonymous market games \citep[See ][~p. 2296]{alger_homo_2013}. While this result is significant, \citep{alger_homo_2013} makes no
mention of how likely it is that a state representing \textit{Homo Hamiltonensis} is reached in a freely and dynamically
evolving model,or under what conditions this may be likely to occur.

\np Interesting work has been undertaken exploring the result achieved by \citet{alger_homo_2013}, one example of this
is \citet{newton_preferences_2017}. Newton holds that the assumption in \citet{alger_homo_2013} that the assortativity
of matching is exogenous and type independent is "very strong" \citep{newton_preferences_2017}. Newton discusses an area
of literature that is interested in the cultural or biological determination of assortativity in which the
so called index of assortativity is subject to evolution. When the evolution of assortativity is considered, Newton
claims, the result of \citet{alger_homo_2013} no longer holds. While Newton concedes that if the rate of mutation of
assortativity was much slower than that of preference the result may hold, he claims that situations in which this is
the case are "difficult to conceive" \citep{newton_preferences_2017}.

\np Returning to the discussion of the synthesis of game theory and computer modelling, \citet{peysakhovich_prosocial_2017} employed agent based models and deep reinforcement learning to draw conclusions about the
performance of artificial agents with social preference in Stag Hunt games. \citet{peysakhovich_prosocial_2017} focuses
on practical considerations surrounding the formation "pro-social" preferences in agents. This is done so by introducing
agents with preferences that are already prosocial and observing the effect this has on the wider population. The paper
concludes that introducing prosocial agents into a population of other reactive learning agents increases (a) the
individual agent's long-term payoff and (b) the likelihood that the group converges to cooperative outcomes.
The Deep reinforcement learning method used in \citet{peysakhovich_prosocial_2017} has the learner treat other agents as
part of the static environment and optimises its own payoff. The authors also discuss other work in the field in which
agents are aware that others are also learning and that their behaviour can influence their opponent's future behaviour
(e.g Babes et al. as seen in \citealt{peysakhovich_prosocial_2017}). The use of these techniques by 
\citet{peysakhovich_prosocial_2017} resulted in insightful findings and their use in this case could be used to inform any reinforcement learning based model that is implemented in extension of existing static models.


% \todo{talk about julian?}

\section{Conclusion}\label{conc}
The literature on the formation of social preferences is rich and varied. With particular interest paid to the formation
of cooperative preferences, the literature reviewed in this paper spans the outlining of basic behavioural analysis in
games and strategies to state-of-the-art findings in static evolutionary game theory. An important point in the
literature is the definition of preferences as a bridge between subjective desire and rational behaviour. 

\np While not entirely novel, there remains much unexplored territory in the dynamic modelling of evolutionary models
initially defined within a context of static analysis. The existing work at the intersection of computational modelling
and game theoretic analysis reviewed in this piece could be viewed as instructive for future work in the area. A useful
characterization of the distinction between the two models is that as content based and value based. Agent based models
are one useful technique to model systems of preference formation. ABMs offer the ability do encode simple and
individualised behaviour at the low level before drawing macro-level observations about the system as a whole.

\np The literature reviewed in this piece that is concerned with game theory and static modelling directly has hopefully
highlighted the space still left to fill in this area of enquiry, while the discussions of dynamic techniques are offered as a method by which this space can be contributed to.
 
\newpage
\bibliography{lit_review}
\bibliographystyle{apalike}
\end{document}