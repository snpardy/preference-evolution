\documentclass[11pt]{article}

\usepackage{chicago}
\usepackage{xcolor}
\usepackage[T1]{fontenc}
\usepackage{fourier}
\usepackage[utf8]{inputenc}
\usepackage[hidelinks]{hyperref}
\usepackage{slashbox}
\usepackage{graphicx}
\usepackage{url}


\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}\PackageWarning{TODO:}{#1!}}
\newcommand{\note}[1]{\textcolor{red}{[NOTE: #1]}\PackageWarning{NOTE:}{#1!}}
\newcommand*{\np}{\par\noindent\newline}

\title{Agent Based Modelling of the Formation of Social Preferences}
\author{S Pardy}
\begin{document}
\begin{titlepage}
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for horizontal lines, change thickness here
	
	\center % Centre everything on the page
	
	%------------------------------------------------
	%	Headings
	%------------------------------------------------
	
	\textsc{\LARGE Monash University}\\[1.5cm]	
	\textsc{\Large Bachelor of Science (Honours)}\\[0.5cm] % Major heading such as course name
	
	\textsc{\large Computational Science}\\[0.5cm] % Minor heading such as course title
	
	%------------------------------------------------
	%	Title
	%------------------------------------------------
	
	\HRule\\[0.4cm]
	
	{\huge\bfseries Agent Based Modelling of the Formation of Social Preferences}\\[0.4cm]
	
    \HRule\\[1.5cm]
    

	
	{\huge Literature Review }\\[0.4cm]
	
    % \HRule\\[1.5cm]
    
    

   \vfill\vfill

	%------------------------------------------------
	%	Author(s)
	%------------------------------------------------
	
	\begin{minipage}{0.4\textwidth}
		\begin{flushleft}
			\large
			\textit{Author}\\
			Sam \textsc{Pardy}
		\end{flushleft}
	\end{minipage}
	~
	\begin{minipage}{0.4\textwidth}
		\begin{flushright}
			\large 
			\textit{Student ID}\\
		25940783   
		\end{flushright}
	\end{minipage}
	\vfill
	\vfill
	{\large
	\textit{Supervisor}\\
	Dr. Julian \textsc{Garcia}}
	
	%------------------------------------------------
	%	Date
	%------------------------------------------------

	\vfill\vfill\vfill % Position the date 3/4 down the remaining page
	{\large\today} % Date
	\vfill % Push the date up 1/4 of the remaining page
	
\end{titlepage}
\hypersetup{colorlinks, citecolor=black, filecolor=black, linkcolor=black urlcolor=black}

\newpage
\tableofcontents
\newpage
\section{Introduction}\label{intro}
Understanding cooperation is greatly important to our understanding of why people behave the way they do.
Understanding the kinds of environments that lead agents to behave cooperatively will allow us to create systems that
encourage or enforce this behaviour. The fact that some of the biggest problems facing humanity require cooperation in
support of solutions is one of the factors that pushes the quest to understand cooperation from one of purely
intellectual value to a thing of great importance to the designers of public policy. Despite the substantial amount of
existing research in the area, understanding cooperation is still an open question.
\np The aim of this research is to explore the results that Alger and Weibull achieve in \cite{alger_homo_2013} in a
dynamic model and so it is necessary to explore the existing literature surrounding both \cite{alger_homo_2013} and the
dynamic modelling of similar representations of systems of potential cooperation.

\np Broadly, I'll begin this literature review with a discussion of the techniques used widely in statically
modelling social interactions, before shifting to a review of the literature surrounding dynamic, agent based modelling.
Finally, we'll look at the existing work done that synthesises these two areas as this is the space that the current
research intends to add to.
\section{Game Theoretic Background}\label{social}
\subsection{Games}
In order to think about and investigate the situations in which agents might act cooperatively, we need to be able to
model the interactions themselves. These 'social' interactions can be characterized as any interaction in which the
payoff of one player is dependent on the action of one or more other players, as well as their own
\cite{angner_course_2012} Game Theory is the study of such social interactions. More formally, according to
\cite{osborne_course_1994} a game includes:
\begin{itemize}
	\item the finite set, N, of players;
	\item for each player, i $\in$ N, a set $A_i$, of actions available to that
	player $N_i$;
	\item and, for each player, i $\in$ N, the set $P_i$ of pay-offs associated
	with the available actions.
\end{itemize}
It is assumed that the players are motivated solely by the payoffs on offer and, in pursuit of these payoffs, act
rationally.
\np One of the most common games used when investigating cooperation is the Prisoner's Dilemma. 
The Prisoner's Dilemma is described as as a situation in which two prisoners are being separately questioned in the hope
that they turn on one another. If neither talk they will both receive a minimal jail sentence, while if one talk
(defects) and the other stays silent (cooperates) the defector will go free and the cooperator will receive the maximum
sentence. If both talk, they will both receive a medium length sentence.  The payoff structure is as follows:
\begin{center}
	\begin{tabular}{|l||*{5}{c|}}\hline
	 \label{prisoner_payoff}
	 \backslashbox{Prisoner A}{Prisoner B}
	 &\makebox[7em]{Cooperate}&\makebox[7em]{Defect}\\\hline\hline
	 Cooperate & 3, 3 & 0, 5\\\hline
	 Defect & 5, 0 & 1, 1 \\\hline
	 \end{tabular}
 \end{center}\mbox{}\\
 Influential game theorist Robert Axelrod holds that examples of the Prisoner's Dilemma can be seen in the military
 decisions of the cold war United States and Soviet Union as well as in their respective decisions regarding defence budgets \cite{axelrod_effective_1980}. Axelrod analyses the Prisoner's Dilemma as capturing the "tension between individual rationality... and group rationality" \cite[p. 4]{axelrod_effective_1980}. This is captured in payoff structure by incentivising both sides to be selfish while also awarding high payoff to both sides when they cooperate.

\np When we talk about how agents play these games, we talk about 'strategies'. A strategy is a complete plan of how to
play the game that includes actions under each possible circumstance \cite{angner_course_2012}. Some terminology used
when discussing strategies: a strategy of the kind outlined above is often called a \textit{pure} strategy, while a
player implementing a \textit{mixed} strategy will play each of the pure strategies with some probability (this
probability could be zero for some strategies). So, in the Prisoner's Dilemma there are two pure strategies available to
each player: Stay Silent or Defect, alternatively, Cooperate or Defect. A mixed strategy would playing each of these two
strategies with some probability.

\np The next step in the analysis is to attempt to predict what strategies players will play when encountering one
another in a game; the way that this is done is by finding \textit{Nash Equilibria}. If, when two players are engaged in
a game, neither player can improve their payoff by changing their strategy while their opponent's strategy remains the
same the players are said to be playing a Nash Equilibrium \cite{angner_course_2012}. Given our assumption about players
acting rationally, by extension, it can be concluded that, when games are repeated, play will end up in a state of Nash
Equilibria \cite{kalai_rational_1993}. In the Prisoner's Dilemma, there is only one Nash Equilibria - both defect. One
of the most influential papers in game theoretic literature is \cite{nash_equilibrium_1950}, in which John Nash
concludes that, when mixed strategies are permitted, any game with a finite strategy set will have at least one equilibrium point. So, when we're analysing the various games in pursuit of cooperation, we can be sure that an equilibrium exists.

\np It should be clear from the discussion so far that any model of the Prisoner's Dilemma that we set up using the
tools mentioned so far will lead to un-cooperative behaviour. Two rational players, motivated by payoffs will engaged in
a Prisoner's Dilemma will eventually play the Nash-Equilibrium strategy of both-defect. So how do reconcile such a model
with cooperative behaviour? In the literature this is done by introducing preferences. In this context, a preference is
a relation between two things \cite{angner_course_2012}; for example, Jane \textit{prefers} Coke to Pepsi. Of course,
preferences such as this are subjective - Jill might prefer Pepsi to Coke - but since our agents are rational, they must
also be rational about their preferences. When this is the case then preferences are transitive and this allows us to
order all alternatives in a list from most preferred to least. This is trivial when an example is given - If Jane
prefers Coke to Pepsi, and she prefers Pepsi to water then we can give a preferences ordering of Coke, Pepsi, water.
Preferences are integral to this area of research as they allow us to bridge the gap between subjectivity and
rationality; a rational agent can have subjective preferences that may go against their objective self interest while
rationally pursuing the outcomes that they most prefer.

\np In the pursuit of cooperation, we are interested in `social' preferences. A preference is considered to be social if
it is a relation that considers the satisfaction of others as well (or even in place of) the satisfaction gained by
oneself \cite{angner_course_2012}. Another way to phrase this definition is to say that an agent with social preferences
is motivated by something other than pure self-interest. Traditionally, much of economics has been predicated on the
assumption that rational actors are motivated solely by self interest, but it has been shown that this analysis fails to
explain large portions of observed behaviour that can be explained when social preferences are considered
\cite{fehr_why_2002}. Being motivated by more than just self-interest is not necessarily a `good' thing though - the
definition says nothing about whether a social preference assigns positive (altruistic) or negative (jealous) weight to
another agents satisfaction. In fact, a mixture of the two, known as reciprocal fairness, is an important preference
type \cite{fehr_why_2002,guth_evolutionary_1995}. An agent who is said to act according to reciprocal fairness
altruistically rewards those who cooperate while also going out of their way to (i.e. reducing their own satisfaction)
to punish those who defect; this second part is often called altruistic punishment \cite{fehr_altruistic_2002}.

\np The way that we represent preferences numerically is with utility. The logic being that if an agent prefers one
alternative to another, then they will achieve a higher utility when their prefer alternative eventuates. So, each
alternative in a particular set can be determined as having some level of utility associated with it based on the
agent's preference ordering. This determination can be done via a utility function \cite{angner_course_2012}. A utility
function maps from objective, material outcomes to the subjective utility that an agent achieves when that outcome
occurs. So, given a two player game and the utility functions of both players we can use these utility functions
to write down a new game that is structured around the utility that each player receives in each potential outcome
rather that the payoff outcome itself.

\np For example, let's assume we have agents who have social preferences about the outcome of the game they're playing.
In the following utility function, the player is some kind of utilitarian and cares about their opponent as much as themselves.
\begin{center}
	\(U(P_i, P_o) = \frac{1}{2} \times P_i + \frac{1}{2} \times P_o\) 
\end{center}
Where $U$ is the utility achieved, $P_i$ is my payoff and $P_o$ is my opponent's payoff.
If we assume that both players in a Prisoner's Dilemma have this utility function, the payoff structure is significantly
changed. The \textit{utility} based payoff matrix now looks like:
\newline
\begin{center}
	\begin{tabular}{|l||*{5}{c|}}\hline
		\label{utility_prisoner}
		\backslashbox{Prisoner A}{Prisoner B}
		&\makebox[7em]{Cooperate}&\makebox[7em]{Defect}\\\hline\hline
		Cooperate & 4.5, 4.5 & 2.5, 2.5\\\hline
		Defect & 2.5, 2.5 & 1, 1 \\\hline
	\end{tabular}
\end{center}\mbox{}\\
Now, unlike the original Prisoner's Dilemma, both players cooperating is an equilibrium point.

\np It's here though, that we run into a problem. The players are now engaging in a game of incomplete information.
In games where players only care about the payoff that they achieve it is often reasonable to assume that the players
also know about the payoffs that their opponents might receive. In other words, the players know about the game that
they're playing. When we take into account their subjective preferences, it does not seem plausible to assume that the
players would know about each others internal desires. This is important because when deciding which action to take
players must reason about and take into account the action that their opponent will take. It is an open question in the
literature how to resolve this issue. \cite{kalai_rational_1993} holds that even in the presence of incomplete
information, play will end up in an equilibrium when games are repeated and so it is possible that complete information
could be assumed. \todo{say more about Bayesian games and proposed solutions to this problem}

\subsection{Evolutionary Game Theory}\label{evo_games}
Modelling the \textit{formation} of preferences is the purview of evolutionary game theory. Evolutionary game theory (as
the name suggests) is the result of the combination of traditional game theory and theories of biological evolution to
explore that changing of behaviour over time \cite{tanimoto_fundamentals_2015}. The key concepts of evolutionary game
theory are include the key concepts in evolutionary biology: fitness, mutation, selection. When dealing with the
formation of preferences, players action is motivated by their preference but their \textit{fitness} is related to their
objective payoff. \todo{example of utility/payoff distinction}


\np Analogous to the Nash Equilibrium solution concept in pure game theory, is the idea of evolutionary stable
strategies (ESS) in evolutionary game theory. A strategy is said to be evolutionary stable if it achieves a higher
payoff (fitness) than that which is achieved by every mutant strategy when the population share of the mutant strategy
is small \cite{weibull_introduction_1992}. That is, strategy $s$ achieves a higher payoff when playing against itself
and against some mutant strategy $m$ than $m$ does playing in the same environment \cite{shoham_multiagent_2008}. In
\cite{weibull_introduction_1992} Weibull also notes that the concept of evolutionary stable strategies is more
stringent than Nash equilibrium: if a strategy is evolutionary stable it is also a Nash equilibrium response to itself
as well as fulfilling the other requirements that ESS imposes.

\section{Computer Modelling \& Game Theory}\label{agent}
Much of the existing literature surrounding evolutionary game theory is concerned with performing static analysis of
evolutionary models \cite{alger_homo_2013,hetzer_evolutionary_2013,newton_preferences_2017}. So, while performing
computer simulations of evolutionary game theory processes is not an entirely novel area, there remains are a large
array of unexplored territory. A basic and useful technique in this area is \textit{genetic programming (GP)}. Not
limited to evolutionary game theory, GP abstracts the search and optimization processes of evolution to apply them to a
wide range of hard to solve problems. When used as an optimization technique, GP improves the population of potential
solutions generation by generation by stochastically transforming these solutions and assessing there fitness
\cite{poli_field_2008}. The fitness functions referenced in \cite{poli_field_2008} are varied and problem specific, in
our context a fitness function will be based somehow on the payoff that a player achieves. In discussing GP as applied
to game theory, Robert Axelrod described the technique as a "highly effective" method for exploring strategies \cite[p.
~23]{axelrod_effective_1980}. 

\np Another important technique in this area is agent based modelling (ABM). ABM is a family of computational modelling
that attempts to draw macro-level conclusions about systems or environments by modelling actions of individual agents.
ABM is an important modelling technique, not least because rather than describing a system as a whole it describes the
actions that individual agents can take. In this way, ABMs are "less simplified" than other forms of modelling and allow
us to observe potentially more complicated dynamics \cite[p. ~25]{railsback_agent-based_2011}. An ABM is made up of 


\section{Existing Work}
\todo{talk about alger and weibull, peysacovich and julian}
\section{Conclusion}\label{conc}
A conclusion.
\newpage
\bibliography{lit_review}
\bibliographystyle{chicago}
\end{document}