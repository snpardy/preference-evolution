\documentclass[11pt]{article}

\usepackage{chicago}
\usepackage{xcolor}
\usepackage[T1]{fontenc}
\usepackage{fourier}
\usepackage[utf8]{inputenc}
\usepackage[hidelinks]{hyperref}
\usepackage{slashbox}
\usepackage{graphicx}

\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}\PackageWarning{TODO:}{#1!}}
\newcommand{\note}[1]{\textcolor{red}{[NOTE: #1]}\PackageWarning{NOTE:}{#1!}}
\newcommand*{\np}{\par\noindent\newline}

\title{Agent Based Modelling of the Formation of Social Preferences}
\author{S Pardy}
\begin{document}
\begin{titlepage}
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for horizontal lines, change thickness here
	
	\center % Centre everything on the page
	
	%------------------------------------------------
	%	Headings
	%------------------------------------------------
	
	\textsc{\LARGE Monash University}\\[1.5cm]	
	\textsc{\Large Bachelor of Science (Honours)}\\[0.5cm] % Major heading such as course name
	
	\textsc{\large Computational Science}\\[0.5cm] % Minor heading such as course title
	
	%------------------------------------------------
	%	Title
	%------------------------------------------------
	
	\HRule\\[0.4cm]
	
	{\huge\bfseries Agent Based Modelling of the Formation of Social Preferences}\\[0.4cm]
	
	\HRule\\[1.5cm]
	
	%------------------------------------------------
	%	Author(s)
	%------------------------------------------------
	
	\begin{minipage}{0.4\textwidth}
		\begin{flushleft}
			\large
			\textit{Author}\\
			Sam \textsc{Pardy}
		\end{flushleft}
	\end{minipage}
	~
	\begin{minipage}{0.4\textwidth}
		\begin{flushright}
			\large 
			\textit{Student ID}\\
		25940783   
		\end{flushright}
	\end{minipage}
	\vfill
	\vfill
	{\large
	\textit{Supervisor}\\
	Dr. Julian \textsc{Garcia}}
	
	%------------------------------------------------
	%	Date
	%------------------------------------------------
	
	\vfill\vfill\vfill % Position the date 3/4 down the remaining page
	
	{\large\today} % Date

	\vfill % Push the date up 1/4 of the remaining page
	
\end{titlepage}
\hypersetup{colorlinks, citecolor=black, filecolor=black, linkcolor=black urlcolor=black}

\newpage
\tableofcontents
\newpage
\section{Introduction}\label{intro}
Why do people cooperate to the level that we do? Paying your taxes; following
the road rules; these are situations in which it would be beneficial for an
individual to be un-cooperative, yet the vast majority of people cooperate. The
reason why is still unknown. Cooperation is referenced as one of the open
questions of science \cite{pennisi_how_2005}. Understanding the origins and
mechanisms of cooperation is not only of intellectual value it will allow us to
build systems that implement or engender this behaviour. With workplace
automation increasing, human-machine interaction is becoming more important,
and implementing better human-machine cooperation can enhance this process
\cite{hoc_human_2000}. Further, some of the biggest problems facing humanity
(climate change, over-population) require cooperative solutions - understanding
cooperation will help us to design public policy that encourages people to
cooperate in support of solutions.

\np The research proposed herein is intended to contribute to the explanation
of cooperation by addressing questions in \textit{Evolutionary Game Theory}
with a particular focus on the the formation of social preferences that result
in cooperative action amongst agents. This is an interdisciplinary research
area, spanning biology, economics, philosophy and computer science.

\np I first outline the wider research context of game theory, evolutionary
game theory and preferences; as well as agent based modelling. I also discuss
\cite{alger_homo_2013}, which is a result that my research will pay specific
attention to. With the broader research space outlined, I move on to a detailed
discussion of the proposed research including a specific account of the
questions the research intends to answer and the methods used to do so. The
piece concludes with a preliminary timeline that the research project will
adhere to.
\section{Research Context}
\subsection{What is modelling social interactions?}\label{modelling_social_interactions}
Strategic interactions occur when agents interact in a way such that the
outcome of the interaction for one, is dependent not  only on their decision
but on the decision of the agent(s) that they are interacting with
(`playing against'). There is no shortage of examples of interactions that can
be characterized in this way - from playing chess; to investing in the stock
market; to choosing which route to take on your daily commute; to political
parties campaigning during an election.
\np To give an example, when you are playing chess whether your move is a good
one or not depends on the moves your opponent intends to make in future. So,
you take this into account - your move depends on what you think you opponent
is going to do. However, your opponent knows this also, so, their move depends
on what they think you're going to do. So, really your move depends on what you
think your opponent thinks you're going to do. Of course, this recursion can be
followed ad infinitum, so we need some formal way to model these interactions -
this is where Game Theory becomes important.
\subsubsection{Games}
Formally, a person is playing a game in any situation in which the result they
achieve is determined not only by their own decision but by the decisions of
one or more other agents involved \cite{angner_course_2012}. Game theory is the
study of such `strategic' interactions.
The structure of a game includes:
\begin{itemize}
	\item the finite set, N, of players;
	\item for each player, i $\in$ N, a set $A_i$, of actions available to that
	player $N_i$;
	\item and, for each player, i $\in$ N, the set $P_i$ of pay-offs associated
	with the available actions.
\end{itemize}
\cite{osborne_course_1994}
\np Below is an example of the payoff structure of one of the most famous games
that deals with cooperation: 
\paragraph{The Prisoner's Dilemma:}\mbox{}\\
In the Prisoner's Dilemma, two criminals are being interrogated
separately with the view to get them to turn on one another - if they both
remain silent, they will each receive only a small jail sentence but if one
stays silent and the other talks then the one who talks will be released while
the one who doesn't will receive a large jail sentence. However, if they both
talk, blaming the other, then they will both receive a medium jail sentence.
Since the jail sentence each player receives depends on the other player's
action, this is a game! A part of what makes selecting the best action
difficult is that the players decide their action simultaneously and so do not
know how the other will act. The payoff matrix can be written as follows:
\begin{center}
   \begin{tabular}{|l||*{5}{c|}}\hline
	\label{prisoner_payoff}
	\backslashbox{Prisoner A}{Prisoner B}
	&\makebox[7em]{Remain Silent}&\makebox[7em]{Talk}\\\hline\hline
	Remain Silent & -1, -1 & -5, 0\\\hline
	Talk & 0, -5 & -3, -3 \\\hline
	\end{tabular}
\end{center}\mbox{}\\
In the table above, prisoner A (the row player) chooses between the rows and
receives the payoff on the left hand side of the cell while prisoner B (the
column player) chooses between the columns and receives the pay off listed on
the right hand side of the cell.
\np In order to reason about the actions that players will take game theoretic
analysis makes several of assumptions about players:
\begin{enumerate}
    \item They target some objective, material pay-off (e.g. the prisoner wants
    to minimize jail time).
    \item Their decisions in pursuit of this pay-off are rational.
    \item They reason in a way that takes into account their beliefs about
    their opponent's action (e.g. prisoner A knows B will also try to minimise
    jail time). 
\end{enumerate}
\cite{osborne_course_1994}
\np The third assumption here is key - players do not just select the action
that is associated with the maximum potential payoff, they select actions to
maximise their payoff relative to what their opponent will do (in their
opponent's pursuit of maximising their payoff). So, players do not select just
one action to take, rather they play a \textit{strategy} that determines their
action under all possible circumstances.

\subsubsection{Strategies \& Nash Equilibrium}
In game theory, players are assumed to prefer the maximum personal, objective
pay-off. So, when choosing between two (or more) options a player only cares
about her own objective pay off and tries to maximise this. The `options' that
the player selects in a given strategic interaction are known as strategies.
Strategies should not be confused with `moves' or `actions'. A strategy is not
a single move, rather, it's an exhaustive delineation of how a player will act
in any given situation in the game. 

\np The proposed solution to the kinds of strategic games we are interested in
is a Nash Equilibrium \cite{angner_course_2012}. Nash Equilibrium is achieved
if, when both players have chosen a strategy, no player can gain anything by
changing their strategy, while their opponent keeps their strategy unchanged.
Returning to the Prisoner's Dilemma outlined in \ref{prisoner_payoff}, this
game has only one Nash Equilibrium: both talk. This is because if prisoner A
remains silent, prisoner B will achieve the highest payoff by talking. But if
prisoner B is to talk, A would be better served by changing her response to
talk also. When both players decide to talk, neither can improve their payoff
by changing to remain silent.

\subsubsection{With Preferences}
In the real world, it is difficult to explain all the decisions that people
make if their actions are analysed as being motivated only by their own
material payoff. To include more complex motivations we can introduce
preferences. Preferences map the payoff a player achieves in a game to a
utility function. So, in a given situation, rather than attempting to maximise
payoff players attempt to maximise utility. Another way to describe
preferences, is that they include, as part of the motivation for action, more
than just objective payoff. For example, buying and consuming a can of soft
drink is something that must be motivated by subjective utility rather than
material payoff. Not only does this \textit{cost} money it is also bad for your
health, in fact, it is difficult to think of any material, objective payoff at
all associated with soft drink. So, even though buying and consuming it is a
net loss in terms of payoff, people buy and consume soft drink all the time.
This must be because people derive some positive \textit{utility} from it,
rather than objective payoff.

\np A preference is considered `social' when a player's utility function is
dependent on their opponent's payoff (or utility). A social preference may
assign positive weight to the opponent's outcome (so our player is "happy" when
their opponent goes well). This is known as an "altruistic" preference. It may
also be the case that the preference assigns negative weight to the opponent's
outcome - so, our player is "envious" when their opponent does well
\cite{angner_course_2012}.
\np Given a two-player game (made up of pay-offs) and the utility functions of
the players, we can write down a \textit{distinct} game based on the utility
each player derives from the pay-offs achieved in each potential result state
in the original game. If we again return to the Prisoner's Dilemma in
\ref{prisoner_payoff}, let's assume our players still care about their own
payoff, while also caring about their opponent's payoff half as much, their
utility function could be:
\begin{center}
\(U = P_i + .5 \times P_o\) 
\end{center}
This simple modification entirely changes the payoff structure of the
Prisoner's Dilemma. The \textit{utility} based payoff matrix now looks like:
\begin{center}
	\begin{tabular}{|l||*{5}{c|}}\hline
	 \label{utility_prisoner}
	 \backslashbox{Prisoner A}{Prisoner B}
	 &\makebox[7em]{Remain Silent}&\makebox[7em]{Talk}\\\hline\hline
	 Remain Silent & -1.5, -1.5 & -5, -2.5\\\hline
	 Talk & -2.5, -5 & -4.5, -4.5 \\\hline
	 \end{tabular}
 \end{center}\mbox{}\\
Suddenly, cooperating (remaining silent) looks much more attractive, and is
also a Nash Equilibrium. But, it is a little complicated. In games in which the
agents only care about pay-offs it is reasonable to assume that player A knows
the pay-off that her opponent, player B, will achieve in each of the possible
outcome states, and she takes this into account when deciding how to act. In
other words, she knows about \textit{the game she is playing}. However, when
preferences are introduced and the game has been transformed to reflect this,
it is not reasonable to assume that player A knows the \textit{utility} that B
will derive from each given outcome. Player A cannot possibly know about B's
inner feelings and desires. So, how then, does A reason about B's action when
making her decision? We come back to this problem later on.

\subsection{Formation of Social Preferences}\label{formation_social_preferences}
It has been purported that cooperation amongst humans is the result of the same
kind of evolutionary forces that shaped cooperation amongst non-human animals
\cite{silk_evolution_2016}. In a nutshell, we assume that the preferences of
people begun as purely selfish (i.e. utility = payoff) and over millennia, were
shaped by Evolution to become prosocial. To model processes such as this, we
can turn to evolutionary game theory. An evolutionary game theory model of
social preference formation includes a population of \textit{agents}, the
majority of which have some utility function r, and are known as Residents.
There is a small portion of the population that known as Mutants who are
motivated by some other utility function, m. The agents in the population are
randomly, pair-wise matched to play some game\footnote{Pair-wise matched,
assuming they are playing a two-player game.}. The utility of the players is
what motivates their actions in the game but the  payoff achieved by each
player contributes to their \textit{fitness} which, in association with the
model's selection process, drives their evolutionary success . When the next
generation is produced, the agents with higher fitness are more likely to
reproduce. In this way, preference formation is driven by evolutionary
forces rather than rationality.So, we can end up in a place in which people
buy cans of soft drink and potentially cooperate when it is not in their
immediate interest to do so.

\subsection{Agent Based Modelling}\label{abm}
Agent based modelling is a kind of computational model that attempts to draw
macro-level conclusions about a system by modelling the actions and behaviours
of individual agents. Agent based modelling is useful in a wide range of areas,
it has been used to evaluate evacuation strategies\cite{taylor_agent-based_2014},
simulate financial markets \cite{deissenberg_eurace:_2008} and investigate
urban sprawl \cite{brown_effects_2006}. We are interested in its applicability
to social preference formation.

\np When we talk about modelling evolution of social preferences using an
agent-based model, we're not in the strict sense talking about an entire
population of individual agents that interact with each other to drive
evolution. Instead, the population structure is what is important - the share
of the population that the resident type occupies and the (much smaller) share
that the mutant type occupies. The individuals that are chosen to interact will
be of resident type or mutant type with some probability according to the
population structure. When drawing conclusions about evolution of preferences
over time, we do so based on changes to the structure of the population (i.e.
what utility function does the resident have?) \cite{shoham_multiagent_nodate}.

\np An important aspect to this analysis is the notion of \textit{evolutionary
stable strategies}. A strategy is considered to be evolutionary stable if it is
resistant to invaders. That is, strategy $s$ is an evolutionary stable strategy
if for any and all mutant strategies, $m'$, that can be introduced to the
population, $s$ achieves a higher payoff playing against itself and $m'$, than
$m'$ does \cite{shoham_multiagent_nodate}.

\np Social preference formation by evolution though,  is not the only way.
Reinforcement learning is a technique in which agents learn from their own
experience and improve their understanding of their environment \textit{within}
their own lifetime, as opposed to evolution in which changes only occur when
the next generation is formed. In RL, agents maintain some representation of
the world - a Q-table or neural network - that includes the possible states the
agent can be in and the expected payoffs achieved for each available action
given their current state. Agents use this representation to attempt to
maximise cumulative payoff. As decisions are made, the agent updates its expected payoffs, thereby, "learning" over time.

\np There is some existing work in this area. \cite{peysakhovich_prosocial_2017}
investigates the development of cooperation between artificial agents in stag
hunt games via deep reinforcement learning. Similar to the research proposed in
in this piece, Peysacovich \& Lerer perform simulations of agent based models
learning over time to explore the formation of cooperative preferences within
artificial agents.

\section{Plan \& Methods}
\subsection{Methods}
The methods used to complete the proposed research will draw from a number of
areas. Broadly, the research structure can be broken down into three distinct
sections: formalising a model; implementing and simulating that model; and
performing mathematical analysis on the results of the simulations.
\np Utilizing the techniques of Evolutionary Game Theory described in
\ref{modelling_social_interactions} and drawing on the analysis undertaken in
\cite{alger_homo_2013} I will outline a game theoretic model of social
preference formation in which agents, motivated by their own preferences, match
to play games and achieve some objective payoff. To shift this to a dynamic
analysis, computer simulations will be performed. The software that will
perform these simulations will be implemented inline with the techniques
described in \ref{abm}, including evolution and reinforcement learning. Another
useful technique in this area is Genetic Programming. Normally a process by
which solutions to hard problems are optimised via evolution, the evolutionary
process by which social preferences will be formed can be thought of as an
instance of genetic programming; these techniques are drawn from
\cite{poli_field_2008}.
\np Once the simulations have been performed (both reinforcement learning and
evolution) we will have data representing both the final preferences or
utility function that was formed by the respective processes and the paths
that were taken to arrive at those preferences. We will then perform some
mathematical analysis on this data to investigate, among other things, the
level of cooperation achieved and how this compares to \cite{alger_homo_2013}.

\subsection{Difficulties \& Required Facilities}
Two main difficulties that are apparent from the outset are related to open
questions in the literature surrounding evolutionary game theory. First, as
referenced earlier, when preferences are taken into account it is reasonable to
expect that players do not know their opponent's preferences; we are now
dealing games of incomplete information, in which players do not have complete,
common knowledge about the game that's being played. If this is the case, players will not be able to compute Nash Equilibrium as they normally
would (by taking into account their opponent's payoff). One potential solution
to this problem is for players to assume that they and their opponent's have
the same preferences or to just assume that the players somehow do know their
opponent's preferences. A more advanced solution is to model players
uncertainty about the game they're playing using Bayesian statistics, these
beliefs can be updated over time with experience. Games of incomplete
information and Bayesian games are discussed in detail in
\cite{shoham_multiagent_nodate}.
\np The second obvious difficulty is deciding how to deal with multiple Nash
Equilibria. A game may have several equilibria, but there is no universal
mechanism for choosing between them. It may be reasonable to have players
randomly select an equilibrium to play when there are multiple options, but
there are more sophisticated potential solutions. One of which is presented in
\cite{binmore_evolutionary_1999}. The model that is eventually implemented to
undertake the research I'm proposing in this piece will need to
settle on, and justify, answers to both the difficulties raised in this section.
\np Running simulations of the kind described is computationally expensive. The
compute power of standard consumer computers will not be sufficient to run the
simulations required for the proposed research. To solve this, we will utilise
Monash's high-performance computing cluster, "MonARCH". MonARCH has nodes of
several different configurations, with the number of cores per server ranging
from 16 to 36. The MonARCH documentation can be viewed in detail at
\cite{michnowicz_monarch_nodate}.
\pagebreak
\subsection{Timeline}
Below is preliminary timeline that I will attempt to adhere to in completing
this research. Figure \ref{fig:main_gantt} shows the timeline of the work that
is to be completed in undertaking the research and figure
\ref{fig:delivery_gantt} shows the milestones and delivery schedule of the
project.
\begin{figure}[!htb]
	\includegraphics[width=\linewidth]{./project_timeline/Slide1.PNG}
	\caption{A Gantt chart displaying the work to be done on the proposed research.}
	\label{fig:main_gantt}
\end{figure}
\begin{figure}[!htb]
	\includegraphics[width=\linewidth]{./project_timeline/Slide2.PNG}
	\caption{A Gantt chart displaying the milestones and delivery dates of the project.}
	\label{fig:delivery_gantt}
\end{figure}
\section{Questions \& Objectives}
In the existing literature, analysis is often done on models in which agents
are matched to play a two player cooperation game in a \textit{uniformly
random} way \todo{add citation}. Firstly, in the real world this is often not
the case - we are more likely to match with some kinds of people than others
(depending on the context we're talking about) and secondly, it has been shown
that the level of assortativity that is present in the matching process has a
large influence on the level of cooperation that is eventually
displayed \cite{alger_homo_2013}.

In \cite{alger_homo_2013} the authors perform a static analysis to
conclude that a specific relationship between assortativity and cooperation is
an evolutionary stable strategy. The analysis undertaken in \cite{alger_homo_2013} makes no mention of how likely it is that this outcome
is reached in a dynamic system that is allowed to freely evolve. So, that begs
the first question that the proposed research will attempt to answer:
\np\textbf{How does the formation of social preferences in a dynamically evolving system compare to the static analysis in \cite{alger_homo_2013}?}
\begin{itemize}
\item How likely is the \cite{alger_homo_2013} result to occur, if at all? 
\item With the linearity imposed in \cite{alger_homo_2013} in mind, what kind
of shaped utility functions does the simulation produce?
\end{itemize}
Another consideration, is that the shifting of agents' preferences can take
several forms. It can be the traditional sense of evolution, in which traits
are passed down from generation to generation mutating and changing over time,
mimicking Darwinian Evolution. Or, agents can learn within their own lifetime,
via reinforcement learning. So, the next area of interest is:
\np\textbf{How does social preference formation driven by evolution compare to that  of reinforcement learning?}
\begin{itemize}
\item Is one closer to the result in \cite{alger_homo_2013}?
\item How similar are the levels of cooperation that tend to result from each
type of preference formation?
\end{itemize}
\bibliography{zotero_thesis}
\bibliographystyle{chicago}
\end{document}